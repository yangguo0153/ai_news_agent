"""Reddit å¸–å­æŠ“å–å·¥å…·

ä½¿ç”¨ PRAW (Python Reddit API Wrapper) æŠ“å–æŒ‡å®š subreddit çš„çƒ­é—¨å¸–å­ã€‚
"""

import os
from datetime import datetime, timezone
from typing import Any, Type, Optional

import praw
import requests
from crewai.tools import BaseTool
from pydantic import BaseModel, Field


class RedditFetchInput(BaseModel):
    """Reddit æŠ“å–å·¥å…·çš„è¾“å…¥å‚æ•°"""
    subreddits: list[str] = Field(
        description="è¦æŠ“å–çš„ subreddit åˆ—è¡¨ï¼Œä¾‹å¦‚ ['LocalLLaMA', 'ArtificialIntelligence']"
    )
    limit: int = Field(
        default=25,
        description="æ¯ä¸ª subreddit æŠ“å–çš„å¸–å­æ•°é‡ä¸Šé™"
    )
    time_filter: str = Field(
        default="day",
        description="æ—¶é—´è¿‡æ»¤å™¨: 'hour', 'day', 'week', 'month', 'year', 'all'"
    )


class RedditFetchTool(BaseTool):
    """æŠ“å– Reddit subreddit çƒ­é—¨å¸–å­çš„å·¥å…·"""
    
    name: str = "fetch_subreddit_posts"
    description: str = (
        "ä»æŒ‡å®šçš„ Reddit subreddit æŠ“å–çƒ­é—¨å¸–å­ã€‚"
        "è¿”å›å¸–å­çš„æ ‡é¢˜ã€é“¾æ¥ã€è¯„åˆ†ã€è¯„è®ºæ•°ç­‰ä¿¡æ¯ã€‚"
        "éœ€è¦æä¾› subreddit åç§°åˆ—è¡¨ã€‚"
    )
    args_schema: Type[BaseModel] = RedditFetchInput
    
    _reddit: Any = None
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._init_reddit()
    
    def _init_reddit(self):
        """åˆå§‹åŒ– Reddit API å®¢æˆ·ç«¯ï¼Œæ”¯æŒä»£ç†é…ç½®"""
        client_id = os.getenv("REDDIT_CLIENT_ID")
        client_secret = os.getenv("REDDIT_CLIENT_SECRET")
        user_agent = os.getenv("REDDIT_USER_AGENT", "AINewsAgent/1.0")
        
        # è·å–ä»£ç†é…ç½®ï¼ˆä¼˜å…ˆçº§ï¼šREDDIT_PROXY_URL > HTTP_PROXY/HTTPS_PROXYï¼‰
        proxy_url = (
            os.getenv("REDDIT_PROXY_URL") or
            os.getenv("HTTPS_PROXY") or
            os.getenv("HTTP_PROXY") or
            os.getenv("https_proxy") or
            os.getenv("http_proxy")
        )
        
        # å¦‚æœé…ç½®äº†ä»£ç†ï¼Œåˆ›å»ºå¸¦ä»£ç†çš„ session
        requestor_kwargs = {}
        if proxy_url:
            print(f"ğŸŒ ä½¿ç”¨ä»£ç†è®¿é—® Reddit: {proxy_url.split('@')[-1] if '@' in proxy_url else proxy_url}")
            session = requests.Session()
            session.proxies = {
                "http": proxy_url,
                "https": proxy_url
            }
            requestor_kwargs = {"session": session}
        else:
            print("âš ï¸  æœªé…ç½®ä»£ç†ï¼Œç›´æ¥è®¿é—® Redditï¼ˆå¯èƒ½æ— æ³•è¿æ¥ï¼‰")
        
        # åˆå§‹åŒ– PRAW å®¢æˆ·ç«¯
        if client_id and client_secret:
            self._reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent=user_agent,
                requestor_kwargs=requestor_kwargs
            )
        else:
            # åªè¯»æ¨¡å¼ï¼ˆæ— éœ€è®¤è¯ï¼‰
            self._reddit = praw.Reddit(
                client_id="placeholder",
                client_secret="placeholder",
                user_agent=user_agent,
                requestor_kwargs=requestor_kwargs
            )
    
    def _run(
        self,
        subreddits: list[str],
        limit: int = 25,
        time_filter: str = "day"
    ) -> str:
        """æ‰§è¡Œ Reddit å¸–å­æŠ“å–"""
        all_posts = []
        
        for subreddit_name in subreddits:
            try:
                subreddit = self._reddit.subreddit(subreddit_name)
                posts = subreddit.top(time_filter=time_filter, limit=limit)
                
                for post in posts:
                    # è®¡ç®—å¸–å­å¹´é¾„
                    created_time = datetime.fromtimestamp(
                        post.created_utc, 
                        tz=timezone.utc
                    )
                    age_hours = (
                        datetime.now(timezone.utc) - created_time
                    ).total_seconds() / 3600
                    
                    post_data = {
                        "subreddit": subreddit_name,
                        "title": post.title,
                        "score": post.score,
                        "num_comments": post.num_comments,
                        "url": f"https://reddit.com{post.permalink}",
                        "external_url": post.url if not post.is_self else None,
                        "selftext": post.selftext[:500] if post.is_self else None,
                        "age_hours": round(age_hours, 1),
                        "flair": post.link_flair_text,
                    }
                    all_posts.append(post_data)
                    
            except Exception as e:
                all_posts.append({
                    "subreddit": subreddit_name,
                    "error": str(e)
                })
        
        # æŒ‰åˆ†æ•°æ’åº
        valid_posts = [p for p in all_posts if "error" not in p]
        valid_posts.sort(key=lambda x: x["score"], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡º
        result = f"# Reddit çƒ­é—¨å¸–å­ ({datetime.now().strftime('%Y-%m-%d %H:%M')})\n\n"
        result += f"å…±æŠ“å– {len(valid_posts)} æ¡å¸–å­\n\n"
        
        for i, post in enumerate(valid_posts, 1):
            result += f"## {i}. [{post['subreddit']}] {post['title']}\n"
            result += f"- ğŸ”¥ Score: {post['score']} | ğŸ’¬ Comments: {post['num_comments']}\n"
            result += f"- ğŸ”— Reddit: {post['url']}\n"
            if post.get("external_url"):
                result += f"- ğŸŒ External: {post['external_url']}\n"
            if post.get("flair"):
                result += f"- ğŸ·ï¸ Flair: {post['flair']}\n"
            if post.get("selftext"):
                result += f"- ğŸ“ Preview: {post['selftext'][:200]}...\n"
            result += f"- â° Posted: {post['age_hours']} hours ago\n\n"
        
        return result
